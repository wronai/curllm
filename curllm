#!/bin/bash
#============================================================================
# curllm - Browser Automation with Local LLM (8GB GPU compatible)
# Author: Softreck (2025)
# Version: 1.0.0
#============================================================================

# Default configuration
CURLLM_API_HOST="${CURLLM_API_HOST:-http://localhost:8000}"
CURLLM_OLLAMA_HOST="${CURLLM_OLLAMA_HOST:-http://localhost:11434}"
CURLLM_MODEL="${CURLLM_MODEL:-qwen2.5:7b}"
CURLLM_BROWSERLESS="${CURLLM_BROWSERLESS:-false}"
CURLLM_DEBUG="${CURLLM_DEBUG:-false}"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Version info
VERSION="1.0.0"

# Parse command line arguments
METHOD="GET"
URL=""
DATA=""
HEADERS=()
OUTPUT_FILE=""
VERBOSE=false
VISUAL_MODE=false
STEALTH_MODE=false
CAPTCHA_SOLVER=false
USE_BQL=false

# Help function
show_help() {
    cat << EOF
curllm - Browser automation with Local LLM support

Usage: curllm [OPTIONS] <URL or INSTRUCTION>

OPTIONS:
    -X, --request METHOD     HTTP method (GET, POST, PUT, DELETE)
    -d, --data DATA         JSON data or natural language instruction
    -H, --header HEADER     Add header (can be used multiple times)
    -o, --output FILE       Save output to file
    -v, --verbose           Verbose output
    
AUTOMATION OPTIONS:
    --visual                Enable visual mode (screenshots + vision analysis)
    --stealth              Enable stealth mode (anti-bot detection)
    --captcha              Enable CAPTCHA solving
    --bql                  Use BQL (Browser Query Language) mode
    --model MODEL          LLM model to use (default: qwen2.5:7b)
    
SERVICE OPTIONS:
    --start-services       Start required services (Ollama, API server)
    --stop-services        Stop all services
    --status              Check service status
    --install             Install dependencies
    
EXAMPLES:
    # Simple web extraction
    curllm "https://example.com" -d "extract all email addresses"
    
    # Complex workflow with authentication
    curllm -X POST --visual --stealth \\
        -d '{"instruction": "Login and download invoice", 
             "credentials": {"user": "john", "pass": "secret"}}' \\
        https://app.example.com
    
    # BQL mode for structured extraction
    curllm --bql -d 'query { page(url: "https://example.com") { 
        title text links { href text } }}' 

    # With CAPTCHA support
    curllm --visual --captcha -d "Fill form and submit" https://form.example.com

EOF
    exit 0
}

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -X|--request)
            METHOD="$2"
            shift 2
            ;;
        -d|--data)
            DATA="$2"
            shift 2
            ;;
        -H|--header)
            HEADERS+=("$2")
            shift 2
            ;;
        -o|--output)
            OUTPUT_FILE="$2"
            shift 2
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        --visual)
            VISUAL_MODE=true
            shift
            ;;
        --stealth)
            STEALTH_MODE=true
            shift
            ;;
        --captcha)
            CAPTCHA_SOLVER=true
            shift
            ;;
        --bql)
            USE_BQL=true
            shift
            ;;
        --model)
            CURLLM_MODEL="$2"
            shift 2
            ;;
        --start-services)
            start_services
            exit 0
            ;;
        --stop-services)
            stop_services
            exit 0
            ;;
        --status)
            check_status
            exit 0
            ;;
        --install)
            install_dependencies
            exit 0
            ;;
        -h|--help)
            show_help
            ;;
        --version)
            echo "curllm version $VERSION"
            exit 0
            ;;
        *)
            URL="$1"
            shift
            ;;
    esac
done

# Function to check if services are running
check_services() {
    local all_good=true
    
    # Check Ollama
    if ! curl -s "${CURLLM_OLLAMA_HOST}/api/tags" > /dev/null 2>&1; then
        echo -e "${RED}✗ Ollama is not running${NC}"
        echo "  Run: ollama serve"
        all_good=false
    else
        echo -e "${GREEN}✓ Ollama is running${NC}"
    fi
    
    # Check API server
    if ! curl -s "${CURLLM_API_HOST}/health" > /dev/null 2>&1; then
        echo -e "${RED}✗ curllm API server is not running${NC}"
        echo "  Run: curllm --start-services"
        all_good=false
    else
        echo -e "${GREEN}✓ curllm API is running${NC}"
    fi
    
    # Check if model exists
    if command -v ollama > /dev/null 2>&1; then
        if ! ollama list | grep -q "$CURLLM_MODEL"; then
            echo -e "${YELLOW}⚠ Model $CURLLM_MODEL not found${NC}"
            echo "  Run: ollama pull $CURLLM_MODEL"
            all_good=false
        else
            echo -e "${GREEN}✓ Model $CURLLM_MODEL is available${NC}"
        fi
    fi
    
    if [ "$all_good" = false ]; then
        return 1
    fi
    return 0
}

# Function to start services
start_services() {
    echo -e "${BLUE}Starting curllm services...${NC}"
    
    # Start Ollama if not running
    if ! pgrep -x "ollama" > /dev/null; then
        echo "Starting Ollama..."
        ollama serve > /tmp/ollama.log 2>&1 &
        sleep 2
    fi
    
    # Pull model if needed
    if ! ollama list | grep -q "$CURLLM_MODEL"; then
        echo "Pulling model $CURLLM_MODEL..."
        ollama pull "$CURLLM_MODEL"
    fi
    
    # Start API server
    if ! curl -s "${CURLLM_API_HOST}/health" > /dev/null 2>&1; then
        echo "Starting curllm API server..."
        # Find Python script location
        SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
        if [ -f "$SCRIPT_DIR/curllm_server.py" ]; then
            python3 "$SCRIPT_DIR/curllm_server.py" > /tmp/curllm.log 2>&1 &
            echo $! > /tmp/curllm.pid
            sleep 3
        else
            echo -e "${RED}Error: curllm_server.py not found${NC}"
            return 1
        fi
    fi
    
    # Start Browserless if enabled
    if [ "$CURLLM_BROWSERLESS" = "true" ]; then
        if ! docker ps | grep -q "browserless"; then
            echo "Starting Browserless container..."
            docker run -d \
                --name browserless \
                -p 3000:3000 \
                -e "MAX_CONCURRENT_SESSIONS=10" \
                -e "ENABLE_STEALTH=true" \
                browserless/chrome:latest
        fi
    fi
    
    echo -e "${GREEN}Services started successfully!${NC}"
    check_status
}

# Function to stop services
stop_services() {
    echo -e "${BLUE}Stopping curllm services...${NC}"
    
    # Stop API server
    if [ -f /tmp/curllm.pid ]; then
        kill $(cat /tmp/curllm.pid) 2>/dev/null
        rm /tmp/curllm.pid
        echo "Stopped curllm API server"
    fi
    
    # Stop Browserless
    docker stop browserless 2>/dev/null
    docker rm browserless 2>/dev/null
    
    echo -e "${GREEN}Services stopped${NC}"
}

# Function to check status
check_status() {
    echo -e "${BLUE}=== curllm Service Status ===${NC}"
    check_services
    
    # Show GPU info if nvidia-smi available
    if command -v nvidia-smi > /dev/null 2>&1; then
        echo ""
        echo -e "${BLUE}GPU Status:${NC}"
        nvidia-smi --query-gpu=name,memory.used,memory.total --format=csv,noheader
    fi
}

# Function to install dependencies
install_dependencies() {
    echo -e "${BLUE}Installing curllm dependencies...${NC}"
    
    # Check for Python 3
    if ! command -v python3 > /dev/null 2>&1; then
        echo -e "${RED}Python 3 is required${NC}"
        exit 1
    fi
    
    # Install Python packages
    echo "Installing Python packages..."
    pip3 install --user \
        browser-use \
        langchain \
        langchain-ollama \
        playwright \
        flask \
        flask-cors \
        pillow \
        opencv-python \
        pytesseract \
        aiohttp
    
    # Install Playwright browsers
    echo "Installing Playwright browsers..."
    python3 -m playwright install chromium
    
    # Install Ollama if not present
    if ! command -v ollama > /dev/null 2>&1; then
        echo "Installing Ollama..."
        curl -fsSL https://ollama.ai/install.sh | sh
    fi
    
    # Create config directory
    mkdir -p ~/.config/curllm
    
    echo -e "${GREEN}Installation complete!${NC}"
    echo "Run 'curllm --start-services' to begin"
}

# Main execution
main() {
    # Check if URL or instruction is provided
    if [ -z "$URL" ] && [ -z "$DATA" ]; then
        echo -e "${RED}Error: URL or instruction required${NC}"
        echo "Use 'curllm --help' for usage information"
        exit 1
    fi
    
    # Check services before running
    if ! check_services > /dev/null 2>&1; then
        echo -e "${YELLOW}Services not ready. Checking status...${NC}"
        check_services
        exit 1
    fi
    
    # Build request payload
    PAYLOAD=$(cat <<EOF
{
    "method": "$METHOD",
    "url": "$URL",
    "data": $( [ -z "$DATA" ] && echo '""' || echo "$DATA" ),
    "visual_mode": $VISUAL_MODE,
    "stealth_mode": $STEALTH_MODE,
    "captcha_solver": $CAPTCHA_SOLVER,
    "use_bql": $USE_BQL,
    "model": "$CURLLM_MODEL",
    "headers": $(printf '%s\n' "${HEADERS[@]}" | jq -R . | jq -s .)
}
EOF
    )
    
    # Show verbose output if requested
    if [ "$VERBOSE" = true ]; then
        echo -e "${BLUE}Request:${NC}"
        echo "$PAYLOAD" | jq .
    fi
    
    # Make API request
    RESPONSE=$(curl -s -X POST \
        "${CURLLM_API_HOST}/api/execute" \
        -H "Content-Type: application/json" \
        -d "$PAYLOAD")
    
    # Check for errors
    if [ $? -ne 0 ]; then
        echo -e "${RED}Error: Failed to connect to curllm API${NC}"
        exit 1
    fi
    
    # Process response
    if [ "$VERBOSE" = true ]; then
        echo -e "${BLUE}Response:${NC}"
        echo "$RESPONSE" | jq .
    else
        # Extract result field
        RESULT=$(echo "$RESPONSE" | jq -r '.result // .error')
        echo "$RESULT"
    fi
    
    # Save to file if requested
    if [ -n "$OUTPUT_FILE" ]; then
        echo "$RESPONSE" > "$OUTPUT_FILE"
        echo -e "${GREEN}Output saved to: $OUTPUT_FILE${NC}"
    fi
}

# Handle Ctrl+C gracefully
trap 'echo -e "\n${YELLOW}Interrupted${NC}"; exit 130' INT

# Run main function if not sourced
if [ "${BASH_SOURCE[0]}" = "${0}" ]; then
    main
fi
